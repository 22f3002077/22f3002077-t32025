{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115439,"databundleVersionId":13800781,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.43.3 -q\n!pip install pyarrow==21.0.0 -q\n!pip install peft==0.11.1 -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:11:43.813575Z","iopub.execute_input":"2025-11-29T08:11:43.813855Z","iopub.status.idle":"2025-11-29T08:13:29.088188Z","shell.execute_reply.started":"2025-11-29T08:11:43.813832Z","shell.execute_reply":"2025-11-29T08:13:29.087451Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Download stopwords\nnltk.download('stopwords')\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\n\nemotion_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:29.089550Z","iopub.execute_input":"2025-11-29T08:13:29.090200Z","iopub.status.idle":"2025-11-29T08:13:31.098543Z","shell.execute_reply.started":"2025-11-29T08:13:29.090175Z","shell.execute_reply":"2025-11-29T08:13:31.097831Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id                                               text  anger  fear  joy  \\\n0   0  the dentist that did the work apparently did a...      1     0    0   \n1   1  i'm gonna absolutely ~~suck~~ be terrible duri...      0     1    0   \n2   2  bridge: so leave me drowning calling houston, ...      0     1    0   \n3   3  after that mess i went to see my now ex-girlfr...      1     1    0   \n4   4  as he stumbled i ran off, afraid it might some...      0     1    0   \n\n   sadness  surprise                    emotions  \n0        1         0         ['anger' 'sadness']  \n1        1         0          ['fear' 'sadness']  \n2        1         0          ['fear' 'sadness']  \n3        1         0  ['anger' 'fear' 'sadness']  \n4        0         0                    ['fear']  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n      <th>emotions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>the dentist that did the work apparently did a...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>['anger' 'sadness']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>i'm gonna absolutely ~~suck~~ be terrible duri...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>['fear' 'sadness']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>bridge: so leave me drowning calling houston, ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>['fear' 'sadness']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>after that mess i went to see my now ex-girlfr...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>['anger' 'fear' 'sadness']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>as he stumbled i ran off, afraid it might some...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['fear']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"emotion_counts = train[emotion_cols].sum().sort_values(ascending=False)\nmost_common_emotion = emotion_counts.idxmax()\nmost_common_count = emotion_counts.max()\n\nprint(\"Most common emotion:\", most_common_emotion)\nprint(\"Count:\", most_common_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.099174Z","iopub.execute_input":"2025-11-29T08:13:31.099435Z","iopub.status.idle":"2025-11-29T08:13:31.113935Z","shell.execute_reply.started":"2025-11-29T08:13:31.099417Z","shell.execute_reply":"2025-11-29T08:13:31.113382Z"}},"outputs":[{"name":"stdout","text":"Most common emotion: fear\nCount: 3860\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train['label_count'] = train[emotion_cols].sum(axis=1)\ncount_exactly_2 = (train['label_count'] == 2).sum()\nprint(\"Instances with exactly 2 labels:\", count_exactly_2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.115500Z","iopub.execute_input":"2025-11-29T08:13:31.115736Z","iopub.status.idle":"2025-11-29T08:13:31.132143Z","shell.execute_reply.started":"2025-11-29T08:13:31.115721Z","shell.execute_reply":"2025-11-29T08:13:31.131295Z"}},"outputs":[{"name":"stdout","text":"Instances with exactly 2 labels: 2587\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"joy_sadness_together = ((train['joy'] == 1) & (train['sadness'] == 1)).sum()\nprint(\"Rows where Joy and Sadness occur together:\", joy_sadness_together)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.132926Z","iopub.execute_input":"2025-11-29T08:13:31.133174Z","iopub.status.idle":"2025-11-29T08:13:31.149166Z","shell.execute_reply.started":"2025-11-29T08:13:31.133153Z","shell.execute_reply":"2025-11-29T08:13:31.148630Z"}},"outputs":[{"name":"stdout","text":"Rows where Joy and Sadness occur together: 96\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"surprise_pct = (train['surprise'].sum() / len(train)) * 100\nprint(\"Percentage containing 'Surprise':\", round(surprise_pct, 2), \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.149786Z","iopub.execute_input":"2025-11-29T08:13:31.150086Z","iopub.status.idle":"2025-11-29T08:13:31.166729Z","shell.execute_reply.started":"2025-11-29T08:13:31.150069Z","shell.execute_reply":"2025-11-29T08:13:31.166161Z"}},"outputs":[{"name":"stdout","text":"Percentage containing 'Surprise': 29.28 %\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"max_diff = emotion_counts.max() - emotion_counts.min()\nprint(\"Maximum difference between any two emotion counts:\", max_diff)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.167359Z","iopub.execute_input":"2025-11-29T08:13:31.167629Z","iopub.status.idle":"2025-11-29T08:13:31.183683Z","shell.execute_reply.started":"2025-11-29T08:13:31.167606Z","shell.execute_reply":"2025-11-29T08:13:31.183059Z"}},"outputs":[{"name":"stdout","text":"Maximum difference between any two emotion counts: 3052\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\nmedian_word_length = train['word_count'].median()\nprint(\"Median word length of texts:\", median_word_length)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.184357Z","iopub.execute_input":"2025-11-29T08:13:31.184702Z","iopub.status.idle":"2025-11-29T08:13:31.206890Z","shell.execute_reply.started":"2025-11-29T08:13:31.184667Z","shell.execute_reply":"2025-11-29T08:13:31.206384Z"}},"outputs":[{"name":"stdout","text":"Median word length of texts: 13.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"corr_ang_fear = train['anger'].corr(train['fear'])\nprint(\"Correlation coefficient between anger and fear:\", round(corr_ang_fear, 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.207491Z","iopub.execute_input":"2025-11-29T08:13:31.207669Z","iopub.status.idle":"2025-11-29T08:13:31.244216Z","shell.execute_reply.started":"2025-11-29T08:13:31.207655Z","shell.execute_reply":"2025-11-29T08:13:31.243564Z"}},"outputs":[{"name":"stdout","text":"Correlation coefficient between anger and fear: 0.08\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"before_chars = train['text'].apply(lambda x: len(str(x))).sum()\n\n# Convert to lowercase\ntrain['clean_text'] = train['text'].str.lower()\n\n# Remove punctuation\ntrain['clean_text'] = train['clean_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\nafter_chars = train['clean_text'].apply(lambda x: len(str(x))).sum()\n\nreduction_pct = ((before_chars - after_chars) / before_chars) * 100\nprint(\"Percentage reduction in total character count after removing punctuation:\", round(reduction_pct, 2), \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.247013Z","iopub.execute_input":"2025-11-29T08:13:31.247231Z","iopub.status.idle":"2025-11-29T08:13:31.285147Z","shell.execute_reply.started":"2025-11-29T08:13:31.247217Z","shell.execute_reply":"2025-11-29T08:13:31.284314Z"}},"outputs":[{"name":"stdout","text":"Percentage reduction in total character count after removing punctuation: 3.26 %\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\n# Get all words\nall_words = [word for text in train['clean_text'] for word in text.split()]\nunique_words = set(all_words)\n\nstopword_overlap = unique_words.intersection(stop_words)\ncommon_stopword_pct = (len(stopword_overlap) / len(unique_words)) * 100\n\nprint(\"Percentage of unique words that are stop words:\", round(common_stopword_pct, 2), \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.285858Z","iopub.execute_input":"2025-11-29T08:13:31.286092Z","iopub.status.idle":"2025-11-29T08:13:31.314726Z","shell.execute_reply.started":"2025-11-29T08:13:31.286071Z","shell.execute_reply":"2025-11-29T08:13:31.314095Z"}},"outputs":[{"name":"stdout","text":"Percentage of unique words that are stop words: 1.54 %\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"filtered_words = [w for w in all_words if w not in stop_words and w.strip() != '']\nword_freq = Counter(filtered_words)\n\nmost_common_words = word_freq.most_common(10)\nfifth_most_freq = most_common_words[4][0]  # index 4 â†’ 5th element\n\nprint(\"Top 10 words (excluding stopwords):\", most_common_words)\nprint(\"5th most frequent word:\", fifth_most_freq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.315469Z","iopub.execute_input":"2025-11-29T08:13:31.315936Z","iopub.status.idle":"2025-11-29T08:13:31.330986Z","shell.execute_reply.started":"2025-11-29T08:13:31.315920Z","shell.execute_reply":"2025-11-29T08:13:31.330412Z"}},"outputs":[{"name":"stdout","text":"Top 10 words (excluding stopwords): [('head', 539), ('eyes', 438), ('like', 394), ('back', 365), ('heart', 334), ('one', 323), ('face', 293), ('get', 291), ('time', 271), ('still', 271)]\n5th most frequent word: heart\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nnltk.download('stopwords')\n\n# Load dataset\ntrain = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\nemotion_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.331665Z","iopub.execute_input":"2025-11-29T08:13:31.331936Z","iopub.status.idle":"2025-11-29T08:13:31.556910Z","shell.execute_reply.started":"2025-11-29T08:13:31.331920Z","shell.execute_reply":"2025-11-29T08:13:31.556306Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\nX = vectorizer.fit_transform(train['text'])\ny = train[emotion_cols]\n\nprint(\"Q1) Feature Matrix Dimension:\", X.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.557551Z","iopub.execute_input":"2025-11-29T08:13:31.557716Z","iopub.status.idle":"2025-11-29T08:13:31.748869Z","shell.execute_reply.started":"2025-11-29T08:13:31.557703Z","shell.execute_reply":"2025-11-29T08:13:31.748327Z"}},"outputs":[{"name":"stdout","text":"Q1) Feature Matrix Dimension: (6827, 33896)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"Q2) Feature representation type:\", \"Sparse\" if hasattr(X, \"toarray\") else \"Dense\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.749578Z","iopub.execute_input":"2025-11-29T08:13:31.750070Z","iopub.status.idle":"2025-11-29T08:13:31.754016Z","shell.execute_reply.started":"2025-11-29T08:13:31.750044Z","shell.execute_reply":"2025-11-29T08:13:31.753311Z"}},"outputs":[{"name":"stdout","text":"Q2) Feature representation type: Sparse\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.754818Z","iopub.execute_input":"2025-11-29T08:13:31.755092Z","iopub.status.idle":"2025-11-29T08:13:31.781165Z","shell.execute_reply.started":"2025-11-29T08:13:31.755073Z","shell.execute_reply":"2025-11-29T08:13:31.780387Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=42, max_depth=6)\ndt.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:31.781851Z","iopub.execute_input":"2025-11-29T08:13:31.782152Z","iopub.status.idle":"2025-11-29T08:13:32.044762Z","shell.execute_reply.started":"2025-11-29T08:13:31.782132Z","shell.execute_reply":"2025-11-29T08:13:32.044122Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DecisionTreeClassifier(max_depth=6, random_state=42)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=6, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=6, random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"n_nodes = dt.tree_.node_count\nprint(\"Q3) Number of nodes created:\", n_nodes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.045468Z","iopub.execute_input":"2025-11-29T08:13:32.046189Z","iopub.status.idle":"2025-11-29T08:13:32.049629Z","shell.execute_reply.started":"2025-11-29T08:13:32.046170Z","shell.execute_reply":"2025-11-29T08:13:32.049070Z"}},"outputs":[{"name":"stdout","text":"Q3) Number of nodes created: 55\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_macro_f1 = np.mean([\n    f1_score(y_train.iloc[:,i], y_train_pred[:,i]) for i in range(len(emotion_cols))\n])\ntest_macro_f1 = np.mean([\n    f1_score(y_test.iloc[:,i], y_test_pred[:,i]) for i in range(len(emotion_cols))\n])\n\nprint(\"Q4) Macro F1 on training split:\", round(train_macro_f1, 4))\nprint(\"Q5) Macro F1 on test split:\", round(test_macro_f1, 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.050645Z","iopub.execute_input":"2025-11-29T08:13:32.050875Z","iopub.status.idle":"2025-11-29T08:13:32.096083Z","shell.execute_reply.started":"2025-11-29T08:13:32.050854Z","shell.execute_reply":"2025-11-29T08:13:32.095525Z"}},"outputs":[{"name":"stdout","text":"Q4) Macro F1 on training split: 0.2147\nQ5) Macro F1 on test split: 0.1976\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')\nX_test_final = vectorizer.transform(test_data['text'])\ny_test_pred_final = dt.predict(X_test_final)\n\nsubmission = pd.DataFrame(y_test_pred_final, columns=emotion_cols)\nsubmission.insert(0, 'id', test_data['id'])\nsubmission.to_csv('decision_tree_tfidf_submission.csv', index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.096723Z","iopub.execute_input":"2025-11-29T08:13:32.096954Z","iopub.status.idle":"2025-11-29T08:13:32.151522Z","shell.execute_reply.started":"2025-11-29T08:13:32.096935Z","shell.execute_reply":"2025-11-29T08:13:32.150962Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   id  anger  fear  joy  sadness  surprise\n0   0      0     1    0        0         0\n1   1      0     1    0        0         0\n2   2      0     1    0        0         0\n3   3      0     1    0        0         0\n4   4      0     1    0        0         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsent_1 = \"I am very happy and excited today!\"\nsent_2 = \"I am feeling joyful and thrilled!\"\n\ntfidf = TfidfVectorizer(stop_words='english')\nvectors = tfidf.fit_transform([sent_1, sent_2])\n\ncos_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\nprint(\"Q7) Cosine similarity:\", round(cos_sim, 3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.152166Z","iopub.execute_input":"2025-11-29T08:13:32.152431Z","iopub.status.idle":"2025-11-29T08:13:32.167836Z","shell.execute_reply.started":"2025-11-29T08:13:32.152408Z","shell.execute_reply":"2025-11-29T08:13:32.167289Z"}},"outputs":[{"name":"stdout","text":"Q7) Cosine similarity: 0.0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"Q9) No â€” TF-IDF space is high-dimensional and sparse; KNN performs poorly.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.168487Z","iopub.execute_input":"2025-11-29T08:13:32.168777Z","iopub.status.idle":"2025-11-29T08:13:32.181597Z","shell.execute_reply.started":"2025-11-29T08:13:32.168755Z","shell.execute_reply":"2025-11-29T08:13:32.181042Z"}},"outputs":[{"name":"stdout","text":"Q9) No â€” TF-IDF space is high-dimensional and sparse; KNN performs poorly.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Select rows where anger or fear = 1\nanger_fear_list = list(train[(train['anger'] == 1) | (train['fear'] == 1)]['text'])\n\n# Pick two example sentences\nsent_1 = anger_fear_list[0]\nsent_2 = anger_fear_list[5]\n\nprint(\"Sentence 1:\", sent_1)\nprint(\"Sentence 2:\", sent_2)\n\n# Compute TF-IDF vectors\ntfidf = TfidfVectorizer(stop_words='english')\nvectors = tfidf.fit_transform([sent_1, sent_2])\n\n# Compute cosine similarity\ncos_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\nprint(\"\\nQ7) Cosine Similarity:\", round(cos_sim, 3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.182131Z","iopub.execute_input":"2025-11-29T08:13:32.182323Z","iopub.status.idle":"2025-11-29T08:13:32.203883Z","shell.execute_reply.started":"2025-11-29T08:13:32.182310Z","shell.execute_reply":"2025-11-29T08:13:32.203192Z"}},"outputs":[{"name":"stdout","text":"Sentence 1: the dentist that did the work apparently did a lousy job as in just a few years my teeth decayed under the crowns so i had no choice but to get partials.\nSentence 2: i notice the small surprises and accidents in life and when pointed out, i get stares like i am a beast with leaves in my mouth.\n\nQ7) Cosine Similarity: 0.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load training data\ntrain_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\")\n\n# Emotion labels (multi-label)\nEMOTIONS = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n\n# Simple text cleaning function\ndef clean_text(text):\n    text = text.lower()                                           # convert to lowercase\n    text = re.sub(r\"http\\S+\", \"\", text)                          # remove URLs\n    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)                      # remove punctuation and numbers\n    text = re.sub(r\"\\s+\", \" \", text).strip()                     # remove extra spaces\n    return text\n\n# Apply cleaning\ntrain_df[\"clean_text\"] = train_df[\"text\"].apply(clean_text)\n\n# Train-validation split (80% training / 20% validation)\nX_train, X_val, y_train, y_val = train_df[\"clean_text\"], train_df[EMOTIONS], train_df[EMOTIONS], train_df[EMOTIONS]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df[\"clean_text\"],\n    train_df[EMOTIONS],\n    test_size=0.2,\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:32.204699Z","iopub.execute_input":"2025-11-29T08:13:32.204977Z","iopub.status.idle":"2025-11-29T08:13:36.890068Z","shell.execute_reply.started":"2025-11-29T08:13:32.204952Z","shell.execute_reply":"2025-11-29T08:13:36.889520Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from collections import defaultdict\n\n# Build vocabulary for LSTM scratch model\ndef build_vocab(texts, min_freq=2):\n    vocab = {\"<pad>\": 0}                        # index 0 reserved for padding\n    freq = defaultdict(int)\n\n    # Count frequency of each word\n    for text in texts:\n        for word in text.split():\n            freq[word] += 1\n\n    # Add words occurring â‰¥ min_freq\n    for word, count in freq.items():\n        if count >= min_freq:\n            vocab[word] = len(vocab)\n\n    return vocab\n\nvocab = build_vocab(train_df[\"clean_text\"])\n\n# Tokenizer function â†’ converts text into list of word IDs\ndef tokenizer_simple(text):\n    return [vocab.get(word, 0) for word in text.split()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:36.890719Z","iopub.execute_input":"2025-11-29T08:13:36.890924Z","iopub.status.idle":"2025-11-29T08:13:36.918778Z","shell.execute_reply.started":"2025-11-29T08:13:36.890908Z","shell.execute_reply":"2025-11-29T08:13:36.918278Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=100):\n        self.texts = texts.tolist()\n        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Tokenize text â†’ convert to sequence of word IDs\n        token_ids = self.tokenizer(self.texts[idx])\n\n        # Pad or trim sequence to fixed length\n        token_ids = token_ids[:self.max_len] + [0] * (self.max_len - len(token_ids))\n\n        return torch.tensor(token_ids), self.labels[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:36.919476Z","iopub.execute_input":"2025-11-29T08:13:36.919917Z","iopub.status.idle":"2025-11-29T08:13:36.924567Z","shell.execute_reply.started":"2025-11-29T08:13:36.919894Z","shell.execute_reply":"2025-11-29T08:13:36.923969Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTMEmotion(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):\n        super().__init__()\n\n        # Convert word IDs â†’ vectors\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n        # Bidirectional LSTM â†’ captures left + right context\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n\n        # Final layer outputs 5 emotion logits\n        self.fc = nn.Linear(hidden_dim * 2, 5)\n\n    def forward(self, x):\n        x = self.embedding(x)                # shape: (batch, seq_len, embed_dim)\n        _, (h, _) = self.lstm(x)             # h contains final hidden states (forward + backward)\n\n        # Concatenate forward + backward LSTM hidden states\n        h_final = torch.cat((h[0], h[1]), dim=1)\n\n        # Predict emotions\n        return self.fc(h_final)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:36.925444Z","iopub.execute_input":"2025-11-29T08:13:36.925738Z","iopub.status.idle":"2025-11-29T08:13:36.943012Z","shell.execute_reply.started":"2025-11-29T08:13:36.925714Z","shell.execute_reply":"2025-11-29T08:13:36.942532Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n# ---------------------------\n# DEVICE SELECTION\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------\n# MODEL DEFINITION\n# ---------------------------\nclass LSTMEmotion(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_dim=64, num_labels=5):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_labels)\n\n    def forward(self, x):\n        x = self.embed(x)\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]\n        return self.fc(out)\n\nmodel = LSTMEmotion(len(vocab)).to(device)\n\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------------------------\n# DATASETS + LOADERS\n# ---------------------------\ntrain_ds = TextDataset(X_train, y_train, tokenizer_simple)\nval_ds   = TextDataset(X_val, y_val, tokenizer_simple)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\nval_loader   = DataLoader(val_ds, batch_size=64, num_workers=0)\n\n# ---------------------------\n# DEBUG: TEST THE DATALOADER\n# ---------------------------\nprint(\"\\nTesting if DataLoader works...\")\n\nfor i, batch in enumerate(train_loader):\n    print(\"âœ” First batch loaded successfully!\")\n    break\n\n# ---------------------------\n# EVALUATION FUNCTION\n# ---------------------------\ndef evaluate(model, loader):\n    model.eval()\n    preds, trues = [], []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            logits = model(x)\n            prob = torch.sigmoid(logits)\n            pred = (prob > 0.5).int()\n\n            preds.append(pred.cpu())\n            trues.append(y.cpu())\n\n    preds = torch.cat(preds)\n    trues = torch.cat(trues)\n\n    return f1_score(trues, preds, average=\"macro\")\n\n# ---------------------------\n# TRAINING LOOP\n# ---------------------------\nlog_every = 40\n\nfor epoch in range(15):  # keep 3 epochs for test run\n    model.train()\n    print(f\"\\nğŸ”µ Epoch {epoch+1} starting...\")\n\n    for step, (x, y) in enumerate(train_loader):\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        if step % log_every == 0:\n            pct = (step+1) / len(train_loader) * 100\n            print(f\"  â¤ Step {step+1}/{len(train_loader)} ({pct:.1f}%) | Loss={loss.item():.4f}\")\n\n    f1 = evaluate(model, val_loader)\n    print(f\"âœ… Epoch {epoch+1} â€” Validation Macro F1: {f1:.4f}\")\n\n# ---------------------------\n# SAVE MODEL\n# ---------------------------\ntorch.save(model.state_dict(), \"lstm_model_fast.pth\")\nprint(\"\\nğŸ‰ Training done! Model saved as lstm_model_fast.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:36.946035Z","iopub.execute_input":"2025-11-29T08:13:36.946498Z","iopub.status.idle":"2025-11-29T08:13:47.990460Z","shell.execute_reply.started":"2025-11-29T08:13:36.946481Z","shell.execute_reply":"2025-11-29T08:13:47.989717Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTesting if DataLoader works...\nâœ” First batch loaded successfully!\n\nğŸ”µ Epoch 1 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.6787\n  â¤ Step 41/86 (47.7%) | Loss=0.5640\n  â¤ Step 81/86 (94.2%) | Loss=0.5595\nâœ… Epoch 1 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 2 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5738\n  â¤ Step 41/86 (47.7%) | Loss=0.5768\n  â¤ Step 81/86 (94.2%) | Loss=0.5635\nâœ… Epoch 2 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 3 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5649\n  â¤ Step 41/86 (47.7%) | Loss=0.5844\n  â¤ Step 81/86 (94.2%) | Loss=0.5968\nâœ… Epoch 3 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 4 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5617\n  â¤ Step 41/86 (47.7%) | Loss=0.5856\n  â¤ Step 81/86 (94.2%) | Loss=0.5448\nâœ… Epoch 4 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 5 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5949\n  â¤ Step 41/86 (47.7%) | Loss=0.5919\n  â¤ Step 81/86 (94.2%) | Loss=0.5622\nâœ… Epoch 5 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 6 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5421\n  â¤ Step 41/86 (47.7%) | Loss=0.5940\n  â¤ Step 81/86 (94.2%) | Loss=0.5465\nâœ… Epoch 6 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 7 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5583\n  â¤ Step 41/86 (47.7%) | Loss=0.5623\n  â¤ Step 81/86 (94.2%) | Loss=0.5922\nâœ… Epoch 7 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 8 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5597\n  â¤ Step 41/86 (47.7%) | Loss=0.5654\n  â¤ Step 81/86 (94.2%) | Loss=0.5957\nâœ… Epoch 8 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 9 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5620\n  â¤ Step 41/86 (47.7%) | Loss=0.5970\n  â¤ Step 81/86 (94.2%) | Loss=0.5923\nâœ… Epoch 9 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 10 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5455\n  â¤ Step 41/86 (47.7%) | Loss=0.5619\n  â¤ Step 81/86 (94.2%) | Loss=0.5405\nâœ… Epoch 10 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 11 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5761\n  â¤ Step 41/86 (47.7%) | Loss=0.6179\n  â¤ Step 81/86 (94.2%) | Loss=0.5851\nâœ… Epoch 11 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 12 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5480\n  â¤ Step 41/86 (47.7%) | Loss=0.5519\n  â¤ Step 81/86 (94.2%) | Loss=0.6147\nâœ… Epoch 12 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 13 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5466\n  â¤ Step 41/86 (47.7%) | Loss=0.5431\n  â¤ Step 81/86 (94.2%) | Loss=0.5918\nâœ… Epoch 13 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 14 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5465\n  â¤ Step 41/86 (47.7%) | Loss=0.5595\n  â¤ Step 81/86 (94.2%) | Loss=0.5823\nâœ… Epoch 14 â€” Validation Macro F1: 0.1427\n\nğŸ”µ Epoch 15 starting...\n  â¤ Step 1/86 (1.2%) | Loss=0.5758\n  â¤ Step 41/86 (47.7%) | Loss=0.5579\n  â¤ Step 81/86 (94.2%) | Loss=0.6001\nâœ… Epoch 15 â€” Validation Macro F1: 0.1427\n\nğŸ‰ Training done! Model saved as lstm_model_fast.pth\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================\n# BERT Multi-Label Emotion Classification\n# ============================\nimport os\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATE\"] = \"1\"\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nfrom sklearn.metrics import f1_score\n\n# ---------------------------\n# DEVICE\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------\n# PREPARE TOKENIZER\n# ---------------------------\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenization function\ndef tokenize(batch):\n    return tokenizer(\n        batch[\"clean_text\"],\n        truncation=True,\n        padding=False   # dynamic padding handled by DataCollator\n    )\n\n# ---------------------------\n# METRICS FUNCTION\n# ---------------------------\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = 1 / (1 + np.exp(-logits))      # sigmoid for multi-label\n    preds = (probs > 0.5).astype(int)      # threshold 0.5\n    macro_f1 = f1_score(labels, preds, average=\"macro\")\n    return {\"macro_f1\": macro_f1}\n\n# ---------------------------\n# PREPARE DATASET\n# ---------------------------\ndf2 = train_df.copy()\n# convert labels â†’ float for multi-label classification\ndf2[\"labels\"] = df2[EMOTIONS].astype(float).values.tolist()\n\ndataset = Dataset.from_pandas(df2)\ndataset = dataset.train_test_split(test_size=0.2)\ndataset = dataset.map(tokenize, batched=True)\n\n# ---------------------------\n# LOAD MODEL\n# ---------------------------\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=5,\n    problem_type=\"multi_label_classification\"\n)\nmodel.to(device)\n\n# ---------------------------\n# TRAINING ARGUMENTS\n# ---------------------------\nargs = TrainingArguments(\n    output_dir=\"./bert\",\n    eval_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    fp16=True,          # mixed precision if GPU available\n    report_to=\"none\",\n    run_name=\"bert-pretrained\"\n)\n\n# ---------------------------\n# DATA COLLATOR\n# ---------------------------\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# ---------------------------\n# TRAINER\n# ---------------------------\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# ---------------------------\n# TRAIN MODEL\n# ---------------------------\ntrainer.train()\n\n# ---------------------------\n# SAVE MODEL + TOKENIZER\n# ---------------------------\nmodel.save_pretrained(\"bert_model\")\ntokenizer.save_pretrained(\"bert_model\")\nprint(\"\\nğŸ‰ Model and tokenizer saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:13:47.991175Z","iopub.execute_input":"2025-11-29T08:13:47.991535Z","iopub.status.idle":"2025-11-29T08:16:41.174756Z","shell.execute_reply.started":"2025-11-29T08:13:47.991517Z","shell.execute_reply":"2025-11-29T08:16:41.173980Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 08:13:52.388244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764404032.766617      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764404032.877380      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1884617e2a4b9d9508fc105a4b2b0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58de6b1fa8ae46988f4087e3730b030a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a21efe0fbc843528a96179a8806e315"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb1099b5d50428aba41ed03c49293d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"867f882fd95744b89b6969f856f1eb51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1366 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e29d6310c994f2d883b47a72ccd9c7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df72d863f9da44e383142a55425b12ee"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [684/684 02:22, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.410300</td>\n      <td>0.328169</td>\n      <td>0.736099</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.221700</td>\n      <td>0.277547</td>\n      <td>0.784020</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ‰ Model and tokenizer saved successfully!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nimport pandas as pd\n\n# ------------------------------\n# 1. Convert text â†’ TF-IDF features\n# ------------------------------\ntfidf = TfidfVectorizer(\n    max_features=12000,     # more features = better performance\n    ngram_range=(1,2),      # include bigrams for better emotion capture\n    stop_words='english'\n)\n\nX_tfidf = tfidf.fit_transform(train_df[\"clean_text\"])\n\n# ------------------------------\n# 2. Train one classifier per emotion (multi-label)\n# ------------------------------\nmodels = {}\n\nfor emo in EMOTIONS:\n    clf = LogisticRegression(\n        max_iter=2000,         # increase iterations â†’ more stable training\n        class_weight=\"balanced\", # handles imbalance in emotions\n        solver=\"lbfgs\"\n    )\n    \n    clf.fit(X_tfidf, train_df[emo])\n    models[emo] = clf\n\n# ------------------------------\n# 3. Predict on validation set\n# ------------------------------\nX_val_tfidf = tfidf.transform(X_val)\n\npreds = {}\nfor emo in EMOTIONS:\n    preds[emo] = models[emo].predict(X_val_tfidf)\n\npreds_df = pd.DataFrame(preds)\n\n# ------------------------------\n# 4. Macro F1 Score\n# ------------------------------\nf1 = f1_score(y_val, preds_df, average=\"macro\")\nprint(\"TF-IDF + Logistic Regression Macro F1:\", f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:16:41.175660Z","iopub.execute_input":"2025-11-29T08:16:41.175908Z","iopub.status.idle":"2025-11-29T08:16:42.687216Z","shell.execute_reply.started":"2025-11-29T08:16:41.175891Z","shell.execute_reply":"2025-11-29T08:16:42.686058Z"}},"outputs":[{"name":"stdout","text":"TF-IDF + Logistic Regression Macro F1: 0.8877456926258276\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader\n\n# ---------------------------\n# DEVICE\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------\n# LOAD TEST DATA\n# ---------------------------\ntest_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\")\n\n# ---------------------------\n# LOAD SAVED BERT MODEL\n# ---------------------------\ntokenizer = BertTokenizer.from_pretrained(\"bert_model\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert_model\")\nmodel.to(device)\nmodel.eval()\n\n# ---------------------------\n# TOKENIZE TEST DATA\n# ---------------------------\n# Dynamic padding for efficiency\nencodings = tokenizer(\n    list(test_df[\"text\"]),\n    truncation=True,\n    padding=True,\n    return_tensors=\"pt\"\n)\n\n# Move input tensors to device\ninput_ids = encodings[\"input_ids\"].to(device)\nattention_mask = encodings[\"attention_mask\"].to(device)\n\n# ---------------------------\n# BATCHED INFERENCE\n# ---------------------------\nbatch_size = 32\npredictions = []\n\nwith torch.no_grad():\n    for i in range(0, len(test_df), batch_size):\n        batch_input_ids = input_ids[i:i+batch_size]\n        batch_attention_mask = attention_mask[i:i+batch_size]\n\n        logits = model(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask\n        ).logits\n\n        probs = torch.sigmoid(logits).cpu().numpy()\n        preds = (probs > 0.5).astype(int)\n\n        predictions.extend(preds)\n\n# ---------------------------\n# CREATE SUBMISSION FILE\n# ---------------------------\nsubmission = pd.DataFrame(predictions, columns=EMOTIONS)\nsubmission[\"id\"] = test_df[\"id\"]\nsubmission = submission[[\"id\"] + EMOTIONS]\n\n# Save submission to Kaggle's working directory\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Submission saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:16:42.689093Z","iopub.execute_input":"2025-11-29T08:16:42.689340Z","iopub.status.idle":"2025-11-29T08:16:53.624213Z","shell.execute_reply.started":"2025-11-29T08:16:42.689322Z","shell.execute_reply":"2025-11-29T08:16:53.623613Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nSubmission saved successfully!\n","output_type":"stream"}],"execution_count":31}]}